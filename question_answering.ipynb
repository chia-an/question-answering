{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question answering system\n",
    "\n",
    "In this small project, I implement a question answering system (QAS) with a dual encoder architecture trained on the Ubuntu Dialogue Corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, define the problem\n",
    "\n",
    "There are many types of QAS and they are all different to each other in some way. Some QASs answer questions according to a text corpus in a way similar to a reading comprehension task, some are able to reply to open-ended questions in scenarios such as customer service or conversation. Also, there are different ways for QAS to produce responeses: it can be generated from a generative model, or it can be selected from a predefined set using a retrieval model.\n",
    "\n",
    "After some search, I decide to go with the Ubuntu Dialogue Corpus and build a retrieval model using dual encoder for the following reasons:\n",
    "\n",
    "  * It is more similar to a chatbot or a dialogue system compare to other tasks and datasets.\n",
    "  * There are many literatures and research papers that talk about or use this dataset.\n",
    "  * I find a [preprocessed version](https://drive.google.com/file/d/1VjVzY0MqKj0b-q_KXnaHC49qCw3iDqEY/view) of this dataset so it saves my time.\n",
    "  * A retrieval model is easier to train comparing to a generative one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the preprocessed dataset, dictionary, and a pretrained word embedding\n",
    "\n",
    "I downloade the preprocessed dataset from [here](https://drive.google.com/file/d/1VjVzY0MqKj0b-q_KXnaHC49qCw3iDqEY/view). Following the [original paper](https://arxiv.org/abs/1506.08909) of the dataset, I use GloVe as the pretrained word embedding, which can be downloaded from [here](https://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from utils import load_data, load_dictionary, dataset_to_corpus\n",
    "from consts import data_dir, model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data with sampling ratio 0.05\n",
      "# training data: 50000\n",
      "# validation data: 9780\n",
      "# testing data: 9460\n"
     ]
    }
   ],
   "source": [
    "# Load the splitted data (only use a small subset)\n",
    "train_c, train_r, train_l, \\\n",
    "dev_c, dev_r, dev_l, \\\n",
    "test_c, test_r, test_l = load_data(ratio=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessed dataset is already tokenized and looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "       1766,    3, 1192,  923,    1,   91,   10,    7,   13,    6,    5,\n",
       "         38, 1685,   91,  113,   17,    7,  561,    5,  578,    1,    2,\n",
       "         24,    5,   42,   88,    1,    2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_c[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know the original words, we need to load also the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      " god i hate computers eou why do it have to be so complicate why ca n't it ever be simple eou eot what be your problem eou eot \n",
      "\n",
      "Response:\n",
      " ubuntu be mess up real bad and i 'm be tell i should backup my hd before i try to fix it eou \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dictionary\n",
    "word_index, inv_word_index, MAX_SEQUENCE_LENGTH = load_dictionary()\n",
    "\n",
    "print('Context:\\n', dataset_to_corpus([train_c[0, :]], inv_word_index)[0], '\\n')\n",
    "print('Response:\\n', dataset_to_corpus([train_r[0, :]], inv_word_index)[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eou` and `eot` are specials token that stand for \"end of utterance\" and \"end of turn\", respectively.\n",
    "\n",
    "Because I want to build a retrieval model, I need both \"true response\" and \"false response\" in the dataset to train the model using supervised learning. Here is an example of a false response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True or false response: 0 \n",
      "\n",
      "Context:\n",
      " as you like eou ya get a brand new burner eou becauase your browser doesnt know its sposed to dislay php eou eot well i have add a couple of line to my apache2 conf addtype application x httpd php php eou also a line for phps eou eot i dont know how to make the browser render php eou both eou what libs eou eot http paste ubuntulinux nl 4009 eou eot it be a bad idea to put anything in usr lib eou use usr local lib instead eou eot someone else inform me after i copy p eou but thank p eou eot there be more to it than that eou eot yar i be jump the gun eou will remove var cache debconf config dat do any damage dpkg reconfigure be complain about a lock issue eou eot ah that 's why eou where do you get the game eou eot www racer nl eou eot \n",
      "\n",
      "Response:\n",
      " i have instal them but not sure they get use also it dont include tahoma eou \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_id = 4\n",
    "print('True or false response:', train_l[data_id], '\\n')\n",
    "print('Context:\\n', dataset_to_corpus([train_c[data_id, :]], inv_word_index)[0], '\\n')\n",
    "print('Response:\\n', dataset_to_corpus([train_r[data_id, :]], inv_word_index)[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train the model\n",
    "\n",
    "The model I build is a LSTM dual encoder, i.e. a siamese-like architecture. I experimente with several variations of the model, such as the output dimension of the encoder and the way to compute the prediction from the encoder's outputs.\n",
    "\n",
    "In the original Ubuntu dataset paper, the last layer of the network is $\\sigma(cMr^T+b)$, where $M, b$ are trainable variables and $c, r$ are the output of the encoder of the given context and response, respectively. It is argued in the paper that the transformation $M$ can be seen as a way to generate a response $r'=cM$ and the rest of the formula measures the similarity between $r$ and $r'$. I also try to measure the similarity between $c$ and $r$ directly, i.e. setting $M=I$ and $b=0$.\n",
    "\n",
    "Due to the limited computation power, an normal Intel CPU on a laptop without powerful GPU, I train the model with only a small portion of the dataset, which may affect the performance of neural networks.\n",
    "\n",
    "Please refer to `train.py` for the code. To retrain the model, use the command `$ python3 train.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "Following the original paper of the dataset, I use TF-IDF as a baseline method and evaluate the results using recall@k with different number of false responses. The test set is constructed such that for each context there is one true response and other nine randomly selected false responses. The model ranks the responses based on its output and recall@k computes the number of times the true response appears in the top-k list divided by the total number of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from utils import recall_at_k\n",
    "from tfidf import tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models and their predictions\n",
    "# The prediction files are generated using the script predict.py\n",
    "pred_filenames = {\n",
    "    'DE-no transform (100)': 'dot_0.05/model.2.0.5783862575187761.pred.pkl',\n",
    "    'DE (100)': 'mat_0.05/model.2.0.6053460450503968.pred.pkl',\n",
    "    'DE (50)': 'mat_0.05_dim_50/model.2.0.6308096857158685.pred.pkl',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DE-no transform (100)\n",
      "recall@1 (out of  2): 0.7769556025369979\n",
      "recall@1 (out of 10): 0.3752642706131078\n",
      "recall@2 (out of 10): 0.5750528541226215\n",
      "recall@5 (out of 10): 0.8266384778012685\n",
      "\n",
      "Model: DE (100)\n",
      "recall@1 (out of  2): 0.6976744186046512\n",
      "recall@1 (out of 10): 0.27167019027484146\n",
      "recall@2 (out of 10): 0.4281183932346723\n",
      "recall@5 (out of 10): 0.7526427061310782\n",
      "\n",
      "Model: DE (50)\n",
      "recall@1 (out of  2): 0.7135306553911205\n",
      "recall@1 (out of 10): 0.2727272727272727\n",
      "recall@2 (out of 10): 0.44608879492600423\n",
      "recall@5 (out of 10): 0.7706131078224101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models\n",
    "for model_name, pred_filename in pred_filenames.items():\n",
    "    with open(str(model_dir / pred_filename), 'rb') as f:\n",
    "        y_pred = pickle.load(f)\n",
    "\n",
    "    print('Model:', model_name)\n",
    "    for group_size in [2, 10]:\n",
    "        for k in [1, 2, 5]:\n",
    "            if k >= group_size:\n",
    "                break\n",
    "            r = recall_at_k(y_pred, k, group_size)\n",
    "            print('recall@{} (out of {:2d}): {}'.format(k, group_size, r))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Loading data with sampling ratio 0.05\n",
      "# training data: 50000\n",
      "# validation data: 9780\n",
      "# testing data: 9460\n",
      "Transforming to text corpuses ...\n",
      "Fitting a tfidf model ...\n",
      "Transforming to tfidf features ...\n",
      "Predicting ...\n",
      "Results:\n",
      "\n",
      "recall@1 (1 options): 0.7632135306553911\n",
      "recall@1 (9 options): 0.514799154334038\n",
      "recall@2 (9 options): 0.6088794926004228\n",
      "recall@5 (9 options): 0.7727272727272727\n"
     ]
    }
   ],
   "source": [
    "# The result of TF-IDF\n",
    "tfidf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| . | TF-IDF | DE (100) | DE (50) | DE-no transform (100) |\n",
    "|--|--|--|--|--|\n",
    "| recall@1 (out of 2) | 0.763 | 0.698 | 0.714 | 0.777 |\n",
    "| recall@1 (out of 10) | 0.515 | 0.272 | 0.273 | 0.375 |\n",
    "| recall@2 (out of 10) | 0.609 | 0.428 | 0.446 | 0.575 |\n",
    "| recall@5 (out of 10) | 0.773 | 0.753 | 0.771 | 0.827 |\n",
    "\n",
    "This is the result of the experiment. The three models on the right are dual encoders and the last one has $M=I, b=0$ set in the last layer as described in the previous section. The number in the parentheses is the output dimension of the encoders.\n",
    "\n",
    "We see that all the numbers are a lot greater than a random model would produce, showing that the models are working properly. However, TF-IDF still performes better in most cases. The reason for this is very likely to be the subsampled dataset mentioned in the previous section. In this experiment, I subsample only 5% of the dataset, which reduces the size of the training set from 1,000,000 to 50,000, so that the trainings finish in a reasonable amount of time. But such little data may not be enough for the huge network: a huge word embedding layer with size about 400,000 * 300 and a matrix $M$ of size 100 * 100 at the end. This can be confirmed by comparing between last three columns. Indeed, we see that the performance is improved in all metrics if we reduce the output dimension of the encoder or set $M$ to identity, which both reduce the number of parameters in the model. Reducing the dimension of word embedding might help, but training a new word embedding also needs a lot of data.\n",
    "\n",
    "Regarding the evaluation metric, while it makes sense to use recall@k in this case, it would also be interesting to try some other metrics mentioned in this [paper](https://arxiv.org/abs/1603.08023), such as greedy matching, embedding average and vector extrema, in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:swisscom-qa-env]",
   "language": "python",
   "name": "conda-env-swisscom-qa-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
